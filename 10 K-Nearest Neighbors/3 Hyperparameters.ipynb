{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18634001",
   "metadata": {},
   "source": [
    "##### Hyperparameters\n",
    "\n",
    "focusing on specific parameters will help us move from understanding the theory to tuning a high-performance model.\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. `n_neighbors` (The \"Scope\" Parameter)\n",
    "\n",
    "This is the most impactful hyperparameter. It determines how many neighbors contribute to the prediction.\n",
    "\n",
    "- **Small $k$ (e.g., $1$ or $3$):** Captures local patterns but is highly sensitive to noise and outliers. This leads to **High Variance** (Overfitting).\n",
    "    \n",
    "- **Large $k$ (e.g., $50$ or $100$):** Smoothes out the prediction by looking at a wider area. This leads to **High Bias** (Underfitting), as the model may ignore local nuances.\n",
    "    \n",
    "- **Key Tip:** For classification, always use an **odd number** for $k$ to avoid \"tie votes\" between classes.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "##### 2. `weights` (The \"Influence\" Parameter)\n",
    "\n",
    "This decides whether all neighbors are equal or if \"closer is better.\"\n",
    "\n",
    "- **`uniform` (Default):** Every neighbor gets one equal vote.\n",
    "    \n",
    "- **`distance`:** Closer neighbors have more influence. The weight is usually calculated as $1/\\text{distance}$.\n",
    "    \n",
    "\n",
    "Example Comparison:\n",
    "\n",
    "Imagine you are predicting the price of a house. Your $k=3$ nearest neighbors are:\n",
    "\n",
    "1. **House A:** 10 meters away, price $500k.\n",
    "    \n",
    "2. **House B:** 50 meters away, price $400k.\n",
    "    \n",
    "3. **House C:** 100 meters away, price $400k.\n",
    "    \n",
    "\n",
    "- **Uniform Weight:** $(500 + 400 + 400) / 3 = \\mathbf{\\$433k}$.\n",
    "    \n",
    "- **Distance Weight:** House A is much closer, so it might contribute 70% of the weight, while C contributes only 5%. The prediction will be much closer to **$490k**.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "##### 3. `metric` and `p` (The \"Distance\" Parameters)\n",
    "\n",
    "These define how the \"closeness\" between points is actually calculated. By default, Scikit-Learn uses the **Minkowski distance**, which is a generalized formula controlled by the parameter `p`.\n",
    "\n",
    "|**p Value**|**Distance Metric**|**Best Use Case**|\n",
    "|---|---|---|\n",
    "|**`p=1`**|**Manhattan** ($L_1$ Norm)|When you have many features (high dimensionality) or data follows a grid-like structure (city blocks).|\n",
    "|**`p=2`**|**Euclidean** ($L_2$ Norm)|The standard \"as-the-crow-flies\" distance. Best for physical/spatial data.|\n",
    "|**`p > 2`**|**Minkowski**|Used rarely, but it places even higher \"penalties\" on larger differences between individual features.|\n",
    "\n",
    "---\n",
    "\n",
    "##### Crucial \"Must-Know\" Info\n",
    "\n",
    "> [!IMPORTANT]\n",
    ">\n",
    "> \n",
    "> Feature Scaling is Mandatory: Because KNN relies on distance, features with larger scales (e.g., Salary in $100,000s) will completely drown out features with smaller scales (e.g., Age in 10s) unless you use StandardScaler or MinMaxScaler.\n",
    "\n",
    "\n",
    "> [!TIP]\n",
    ">\n",
    "> \n",
    "> The Curse of Dimensionality: As you add more features (dimensions), the \"distance\" between points becomes less meaningful because everything starts looking far apart. Always try to keep your feature set lean when using KNN.\n",
    "\n",
    "##### Types of Distances\n",
    "\n",
    "In KNN, distance is the \"similarity ruler.\" To understand the specific metrics, it is easiest to look at **Minkowski Distance** first, as it is the mathematical parent of the others.\n",
    "\n",
    "##### 1. Minkowski Distance (The Umbrella)\n",
    "\n",
    "Minkowski distance is a generalized metric. By changing a single parameter, **$p$**, you can transform it into other distance types. It is only considered a \"true\" metric when $p \\ge 1$.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$D(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p}$$\n",
    "\n",
    "- $n$ = number of dimensions (features).\n",
    "    \n",
    "- $x_i, y_i$ = coordinates of the two points in the $i^{th}$ dimension.\n",
    "    \n",
    "- $p$ = the Minkowski parameter (determines the distance type).\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "##### 2. Manhattan Distance ($p=1$)\n",
    "\n",
    "Also known as **Taxicab distance** or **$L_1$ Norm**, it calculates distance as if you were traveling along a grid (like the streets of Manhattan). You can only move horizontally or vertically; no diagonals allowed.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$D(x, y) = \\sum_{i=1}^{n} |x_i - y_i|$$\n",
    "\n",
    "Example:\n",
    "\n",
    "Point A: $(1, 2)$ | Point B: $(4, 6)$\n",
    "\n",
    "- Difference in $x$: $|1 - 4| = 3$\n",
    "    \n",
    "- Difference in $y$: $|2 - 6| = 4$\n",
    "    \n",
    "- **Total Distance:** $3 + 4 = \\mathbf{7}$\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "##### 3. Euclidean Distance ($p=2$)\n",
    "\n",
    "This is the most common metric, often called the **\"as-the-crow-flies\"** distance. It uses the Pythagorean theorem to find the direct straight-line distance between two points.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$D(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$\n",
    "\n",
    "Example:\n",
    "\n",
    "Using the same points A: $(1, 2)$ and B: $(4, 6)$\n",
    "\n",
    "- Square of $x$ difference: $(1 - 4)^2 = 9$\n",
    "    \n",
    "- Square of $y$ difference: $(2 - 6)^2 = 16$\n",
    "    \n",
    "- Sum: $9 + 16 = 25$\n",
    "    \n",
    "- **Total Distance:** $\\sqrt{25} = \\mathbf{5}$\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "##### Comparison Summary\n",
    "\n",
    "|**Metric**|**Minkowski p**|**Logic**|**Best For...**|\n",
    "|---|---|---|---|\n",
    "|**Manhattan**|$1$|Sum of absolute differences|High-dimensional data or discrete/grid-based features.|\n",
    "|**Euclidean**|$2$|Straight line (Pythagorean)|Physical distance or when features are dense and continuous.|\n",
    "|**Chebyshev**|$\\infty$|Maximum difference|Scenarios where only the single largest attribute difference matters.|\n",
    "\n",
    "\n",
    "\n",
    "> **Why it matters:** In KNN, if you use Euclidean distance on high-dimensional data, the \"distance\" can become distorted (all points start looking equally far away). Switching to Manhattan ($p=1$) often yields better results in those cases.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
