{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e754089",
   "metadata": {},
   "source": [
    "### 1. Ridge Regression\n",
    "\n",
    "**Ridge Regression** is a specialized version of linear regression used to create more \"stable\" and reliable models, especially when your data is messy or your model is trying too hard to be perfect.1\n",
    "\n",
    "In technical terms, it is a **regularization technique** (specifically L2 regularization) that prevents a model from **overfitting** by adding a penalty to the size of the coefficients (slopes).2\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Why do we use it?\n",
    "\n",
    "Ordinary Linear Regression (OLS) tries to minimize the distance between the data points and the line. However, OLS can fail in two major scenarios:\n",
    "\n",
    "1. **Overfitting:** When the model is too complex and captures \"noise\" instead of the actual pattern.3 It performs great on training data but fails on new data.\n",
    "    \n",
    "2. **Multicollinearity:** When your input variables are highly correlated (e.g., trying to predict a house price using both \"Square Footage\" and \"Number of Rooms,\" which usually move together).4 This makes the OLS slopes wildly unstable.5\n",
    "    \n",
    "\n",
    "**Ridge Regression solves this** by forcing the slopes to be smaller, which makes the model less sensitive to small fluctuations in the data.6\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Real-Life Example: Predicting House Prices7\n",
    "\n",
    "Imagine you are predicting house prices based on **Square Footage**.\n",
    "\n",
    "- **Ordinary Regression:** To fit every single point perfectly, the model might create a very steep slope.8 If one outlier house is overpriced, the model \"tilts\" heavily to reach it.\n",
    "    \n",
    "- **Ridge Regression:** It says, \"I want to fit the points, but I also don't want my slope to be too extreme.\"9 It introduces a penalty that pulls the slope down.10\n",
    "    \n",
    "\n",
    "#### How to use it (Step-by-Step):\n",
    "\n",
    "1. **Standardize Data:** Because Ridge penalizes the _size_ of coefficients, you must scale your data first (e.g., using Z-score).11 Otherwise, a variable with a large scale (like \"Annual Income\") will be penalized differently than \"Age.\"12\n",
    "    \n",
    "2. **Choose Alpha ($\\alpha$):** This is your \"tuning knob\" for how much you want to penalize the slopes.\n",
    "    \n",
    "3. Train the Model: The model minimizes a new Cost Function:\n",
    "    \n",
    "    $$\\text{Cost} = \\sum(y - \\hat{y})^2 + \\alpha \\sum(\\text{slope})^2$$\n",
    "    \n",
    "    (The first part is the standard error; the second part is the Ridge penalty).13\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. The Relationship Between Alpha ($\\alpha$) and Slope\n",
    "\n",
    "The parameter **$\\alpha$** (also called $\\lambda$ in some textbooks) controls the trade-off between fitting the data and keeping the model simple.\n",
    "\n",
    "|**Alpha (α) Value**|**Effect on Slopes**|**Model Behavior**|\n",
    "|---|---|---|\n",
    "|**$\\alpha = 0$**|No change.|Equivalent to **Ordinary Linear Regression**.|\n",
    "|**Small $\\alpha$**|Slopes are slightly reduced.|Still fits training data well; slightly more stable.|\n",
    "|**Large $\\alpha$**|Slopes shrink significantly.|**Bias increases**, but the model becomes very stable (low variance).|\n",
    "|**$\\alpha \\to \\infty$**|Slopes approach zero.|The line becomes horizontal (a flat average).14|\n",
    "\n",
    "**Key Insight:** As $\\alpha$ increases, the **slopes decrease**. This reduces the model's complexity. Unlike Lasso regression, Ridge will make slopes very, very small (e.g., 18$0.00001$), but it will **never** make them exactly zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Lasso Regression\n",
    "\n",
    "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a powerful \"clean-up\" version of linear regression.1 While Ridge Regression keeps all your variables and just makes them small, Lasso is like a editor—it looks at all your input variables and **deletes** the ones that aren't helping.\n",
    "\n",
    "It is a **regularization technique** (specifically L1 regularization) that prevents overfitting by adding a penalty to the absolute size of the coefficients.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Why do we use it?\n",
    "\n",
    "We use Lasso for two main reasons:\n",
    "\n",
    "1. **Feature Selection (The \"Magic\" of Lasso):** Unlike almost any other regression, Lasso can shrink a variable's slope (coefficient) to **exactly zero**. This effectively removes that variable from the model, making it a great tool for simplifying complex datasets.\n",
    "    \n",
    "2. **Overfitting Prevention:** Just like Ridge, it penalizes models that try to be \"too perfect\" for the training data, ensuring the model generalizes well to new, unseen data.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Real-Life Example: Medical Diagnosis\n",
    "\n",
    "Imagine you are trying to predict a patient's **Blood Sugar Level** using 50 different data points: age, weight, height, favorite color, shoe size, eye color, etc.\n",
    "\n",
    "- **Ordinary Regression:** Will try to use _all_ 50 factors. It might find a weird coincidence where people with \"blue eyes\" and \"size 10 shoes\" have slightly higher blood sugar, so it gives those factors a slope. This is \"noise.\"\n",
    "    \n",
    "- **Lasso Regression:** It realizes that \"favorite color\" and \"shoe size\" have almost zero actual predictive power. The penalty forces their slopes to **exactly zero**, leaving you with a clean, interpretable model that only uses relevant factors like \"weight\" and \"age.\"\n",
    "    \n",
    "\n",
    "#### How to use it:\n",
    "\n",
    "To use Lasso effectively, you must:\n",
    "\n",
    "1. **Standardize your features:** Because Lasso penalizes the magnitude of coefficients, variables with larger numbers (like \"Salary\") would be penalized more than \"Age\" if not scaled.\n",
    "    \n",
    "2. **Choose Alpha ($\\alpha$):** The penalty strength.\n",
    "    \n",
    "3. Minimize the Cost Function:\n",
    "    \n",
    "    $$\\text{Cost} = \\sum(y - \\hat{y})^2 + \\alpha \\sum|\\text{slope}|$$\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. The Relationship Between Alpha and Slope\n",
    "\n",
    "The parameter **Alpha ($\\alpha$)** is the \"intensity\" of the deletion.\n",
    "\n",
    "|**Alpha (α) Value**|**Effect on Slopes**|**Result**|\n",
    "|---|---|---|\n",
    "|**$\\alpha = 0$**|No penalty.|Same as **Standard Linear Regression**.|\n",
    "|**Small $\\alpha$**|Slopes are slightly reduced.|Most features remain; model is slightly more stable.|\n",
    "|**Large $\\alpha$**|Many slopes become **exactly 0**.|**Feature Selection** occurs. The model becomes very simple.|\n",
    "|**$\\alpha \\to \\infty$**|All slopes become 0.|You are left with just the average (the intercept).|\n",
    "\n",
    "##### The Key Difference: Ridge vs. Lasso\n",
    "\n",
    "- **Ridge (L2):** Shrinks slopes toward zero but **never** hits zero. (Keeps everything).\n",
    "    \n",
    "- **Lasso (L1):** Shrinks slopes toward zero and **can** hit exactly zero. (Selects only the best).\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### ElasticNet Regression\n",
    "\n",
    "**Elastic Net Regression** is the \"best of both worlds\" algorithm. It is a regularized regression method that combines the penalties of **Ridge (L2)** and **Lasso (L1)** into a single model.\n",
    "\n",
    "If Ridge is a \"shrinker\" and Lasso is a \"selector,\" Elastic Net is the balanced \"all-rounder\" that knows when to do a little bit of both.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Why do we use it?\n",
    "\n",
    "While Ridge and Lasso are great, they both have specific \"blind spots\" that Elastic Net fixes:\n",
    "\n",
    "- **The Problem with Lasso:** If you have a group of variables that are highly correlated (e.g., \"Daily Steps\" and \"Calories Burned\"), Lasso will often pick one at random and ignore the rest entirely. This can make the model unstable.\n",
    "    \n",
    "- **The Problem with Ridge:** It keeps _all_ variables. If you have 1,000 features and 900 of them are useless, Ridge will still keep them all, making the model messy.\n",
    "    \n",
    "\n",
    "**Elastic Net solves this by:**\n",
    "\n",
    "1. **Grouping Effect:** It keeps or removes highly correlated variables together as a group rather than choosing one randomly.\n",
    "    \n",
    "2. **Flexibility:** It allows you to tune the \"mix\" between Ridge and Lasso depending on your data's needs.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Real-Life Example: Gene Expression Studies\n",
    "\n",
    "Imagine a scientist trying to find which genes (out of 20,000+) are responsible for a specific disease.\n",
    "\n",
    "- **The Challenge:** Many genes work in \"pathways.\" If Gene A is active, Gene B is almost certainly active too (high correlation).\n",
    "    \n",
    "- **How Elastic Net helps:** * **Lasso** might pick only Gene A and discard Gene B, making the scientist miss a whole part of the biological pathway.\n",
    "    \n",
    "    - **Ridge** would keep all 20,000 genes, making it impossible to tell which ones actually matter.\n",
    "        \n",
    "    - **Elastic Net** will realize Gene A and B are a group.11 It will either keep both (if they are important) or remove both (if they aren't), giving the scientist a much more accurate biological picture.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. How to use it: The Two Tuning Knobs\n",
    "\n",
    "To use Elastic Net, you have to adjust two main hyperparameters:\n",
    "\n",
    "1. **Alpha (13$\\alpha$):** This controls the _overall_ strength of the penalty (how much regularization to apply).\n",
    "    \n",
    "2. **L1 Ratio:** This is the \"mixer.\" It decides the percentage of Lasso vs.16 Ridge.\n",
    "    \n",
    "    - **L1 Ratio = 1.0:** The model becomes **Pure Lasso**.\n",
    "        \n",
    "    - **L1 Ratio = 0.0:** The model becomes **Pure Ridge**.\n",
    "        \n",
    "    - **L1 Ratio = 0.5:** A 50/50 split of both.\n",
    "        \n",
    "\n",
    "##### The Elastic Net Equation:\n",
    "\n",
    "$$\\text{Cost} = \\text{MSE} + \\alpha \\cdot \\text{L1\\_Ratio} \\cdot \\sum|\\text{slope}| + \\alpha \\cdot \\frac{1 - \\text{L1\\_Ratio}}{2} \\cdot \\sum(\\text{slope})^2$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary Comparison\n",
    "\n",
    "|**Feature**|**Ridge**|**Lasso**|**Elastic Net**|\n",
    "|---|---|---|---|\n",
    "|**Penalty Type**|L2 (Squares)|L1 (Absolute)|Both (L1 + L2)|\n",
    "|**Feature Selection**|No|Yes|Yes|\n",
    "|**Correlated Variables**|Keeps all|Picks one randomly|Keeps/removes as a group|\n",
    "|**Complexity**|Simple|Simple|Harder to tune (2 params)|"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
